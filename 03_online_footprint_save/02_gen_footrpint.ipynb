{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![QuantConnect Logo](https://cdn.quantconnect.com/web/i/icon.png)\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-02 00:00:00 finished\n",
      "2022-01-03 00:00:00 finished\n",
      "2022-01-04 00:00:00 finished\n",
      "2022-01-05 00:00:00 finished\n",
      "2022-01-06 00:00:00 finished\n",
      "2022-01-07 00:00:00 finished\n",
      "2022-01-09 00:00:00 finished\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m tick_size = sec.SymbolProperties.MinimumPriceVariation\n\u001b[32m     20\u001b[39m data_root=\u001b[33m\"\u001b[39m\u001b[33m/QuantConnect/research-cloud/airlock/footprint_data\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqb\u001b[49m\u001b[43m=\u001b[49m\u001b[43mqb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msymbol\u001b[49m\u001b[43m=\u001b[49m\u001b[43msymbol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m=\u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_unit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mv_unit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtick_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtick_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_recompute\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mdata_root\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_root\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDone.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Lean/Launcher/bin/Debug/Notebooks/orchestrator.py:145\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(qb, symbol, start_date, end_date, v_unit, tick_size, force_recompute, data_root)\u001b[39m\n\u001b[32m    136\u001b[39m     append_no_data_dates(\n\u001b[32m    137\u001b[39m         symbol=symbol,\n\u001b[32m    138\u001b[39m         year=y,\n\u001b[32m   (...)\u001b[39m\u001b[32m    142\u001b[39m         data_root=data_root\n\u001b[32m    143\u001b[39m     )\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m df_v = \u001b[43mbuild_v_footprints\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_unit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mv_unit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtick_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtick_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m df_v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m df_v.empty:\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Lean/Launcher/bin/Debug/Notebooks/footprint_aggregator.py:234\u001b[39m, in \u001b[36mbuild_v_footprints\u001b[39m\u001b[34m(df_second, v_unit, tick_size)\u001b[39m\n\u001b[32m    231\u001b[39m     trade_close = t_c\n\u001b[32m    233\u001b[39m \u001b[38;5;66;03m# micro allocation at second granularity\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m buy_v, sell_v, deltas = \u001b[43mmicro_allocate_volume_raw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mt_o\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mb_o\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_c\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m    \u001b[49m\u001b[43ma_o\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma_c\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtick_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtick_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    241\u001b[39m total_volume_sum += vol\n\u001b[32m    242\u001b[39m buy_volume_sum += buy_v\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Lean/Launcher/bin/Debug/Notebooks/footprint_utils.py:346\u001b[39m, in \u001b[36mmicro_allocate_volume_raw\u001b[39m\u001b[34m(t_o, t_h, t_l, t_c, volume, b_o, b_h, b_l, b_c, a_o, a_h, a_l, a_c, tick_size, alpha, n_min, n_max)\u001b[39m\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tick_size \u001b[38;5;129;01mand\u001b[39;00m tick_size > \u001b[32m0\u001b[39m:\n\u001b[32m    345\u001b[39m     bucket_ints = np.rint(price_path / tick_size).astype(np.int64)\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     uniq, inv = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbucket_ints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m     ask_sums = np.zeros(uniq.size, dtype=\u001b[38;5;28mfloat\u001b[39m)\n\u001b[32m    348\u001b[39m     bid_sums = np.zeros(uniq.size, dtype=\u001b[38;5;28mfloat\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/numpy/lib/arraysetops.py:274\u001b[39m, in \u001b[36munique\u001b[39m\u001b[34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[39m\n\u001b[32m    272\u001b[39m ar = np.asanyarray(ar)\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m     ret = \u001b[43m_unique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[32m    278\u001b[39m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/numpy/lib/arraysetops.py:338\u001b[39m, in \u001b[36m_unique1d\u001b[39m\u001b[34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[39m\n\u001b[32m    336\u001b[39m     ar.sort()\n\u001b[32m    337\u001b[39m     aux = ar\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m mask = np.empty(aux.shape, dtype=np.bool_)\n\u001b[32m    339\u001b[39m mask[:\u001b[32m1\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (equal_nan \u001b[38;5;129;01mand\u001b[39;00m aux.shape[\u001b[32m0\u001b[39m] > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m aux.dtype.kind \u001b[38;5;129;01min\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcfmM\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    341\u001b[39m         np.isnan(aux[-\u001b[32m1\u001b[39m])):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from AlgorithmImports import *\n",
    "from datetime import date\n",
    "import pandas as pd\n",
    "\n",
    "from orchestrator import run\n",
    "from footprint_storage import get_year_file_path\n",
    "\n",
    "qb = QuantBook()\n",
    "symbol = qb.add_future(Futures.Indices.NASDAQ_100_E_MINI, Resolution.SECOND).symbol\n",
    "# Futures.Indices.NASDAQ_100_E_MINI SP_500_E_MINI Futures.Metals.Gold\n",
    "\n",
    "year=2022\n",
    "start = date(year, 1, 1)\n",
    "end = date(year, 12, 31)\n",
    "\n",
    "v_unit = 1000  # minimal volume unit per bar\n",
    "\n",
    "sec = qb.Securities[symbol]\n",
    "tick_size = sec.SymbolProperties.MinimumPriceVariation\n",
    "data_root=\"/QuantConnect/research-cloud/airlock/footprint_data\"\n",
    "run(qb=qb, symbol=symbol, start_date=start, end_date=end, v_unit=v_unit, tick_size=tick_size, force_recompute=False,data_root=data_root)\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从我们刚刚创建的文件中导入校验函数\n",
    "from validator import validate_daily_open\n",
    "\n",
    "\n",
    "# --- 执行 ---\n",
    "print(f\"开始校验合约: {symbol}\")\n",
    "print(f\"日期范围: {start} to {end}\")\n",
    "\n",
    "# ============== Cell 2: 运行校验 ==============\n",
    "print(\"\\n正在运行校验...\")\n",
    "# data_root 参数可以按需修改，这里使用模块中的默认值 '/LeanCLI/footprint_data'\n",
    "validation_errors = validate_daily_open(\n",
    "    qb=qb,\n",
    "    symbol=symbol,\n",
    "    start_date=start,\n",
    "    end_date=end,\n",
    "    data_root=data_root\n",
    ")\n",
    "print(\"校验完成。\")\n",
    "\n",
    "\n",
    "# ============== Cell 3: 显示结果 ==============\n",
    "print(\"\\n--- 校验结果 ---\")\n",
    "if not validation_errors:\n",
    "    print(\"✅ 校验通过！在指定日期范围内，所有日期的开盘价均在2个tick的容忍误差内。\")\n",
    "else:\n",
    "    print(f\"❌ 校验发现 {len(validation_errors)} 个问题。\")\n",
    "    \n",
    "    # 将结果转换为 DataFrame 以便清晰展示\n",
    "    errors_df = pd.DataFrame(validation_errors)\n",
    "    \n",
    "    # 计算差异的tick数量，以便更直观地判断\n",
    "    if \"difference\" in errors_df.columns and \"tick_size\" in errors_df.columns:\n",
    "        # 使用 .loc 避免 SettingWithCopyWarning\n",
    "        errors_df.loc[:, \"difference_in_ticks\"] = errors_df[\"difference\"] / errors_df[\"tick_size\"]\n",
    "    \n",
    "    # 为了更好的可读性，重新排列一下列的顺序\n",
    "    cols_order = [\n",
    "        \"date\", \"status\", \"daily_open\", \"daily_open_time\", \"footprint_open\", \"footprint_open_time\", \n",
    "        \"difference\", \"tick_size\", \"difference_in_ticks\", \"message\"\n",
    "    ]\n",
    "    \n",
    "    # 过滤掉在DataFrame中不存在的列\n",
    "    existing_cols = [col for col in cols_order if col in errors_df.columns]\n",
    "    \n",
    "    print(\"\\n详细信息:\")\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 1000):\n",
    "        print(errors_df[existing_cols].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 第1步：筛选符合正则的文件（支持 max_depth） ===\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import json\n",
    "def find_files(base_dir=\".\", pattern=None, max_depth=None):\n",
    "    \"\"\"\n",
    "    遍历目录并筛选出符合正则的文件\n",
    "    :param base_dir: 要遍历的根目录\n",
    "    :param pattern: 文件名匹配正则表达式\n",
    "    :param max_depth: 最大遍历深度（None 表示不限制）\n",
    "    \"\"\"\n",
    "    base_path = Path(base_dir).resolve()\n",
    "    regex = re.compile(pattern) if pattern else None\n",
    "    matched_files = []\n",
    "\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        # 计算当前深度\n",
    "        depth = len(Path(root).relative_to(base_path).parts)\n",
    "        if max_depth is not None and depth > max_depth:\n",
    "            dirs[:] = []  # 不再深入\n",
    "            continue\n",
    "\n",
    "        for f in files:\n",
    "            fpath = Path(root) / f\n",
    "            if regex is None or regex.search(f):\n",
    "                matched_files.append(fpath)\n",
    "\n",
    "    print(f\"在目录 {base_path} 中找到 {len(matched_files)} 个匹配文件：\")\n",
    "    matched_files = sorted(matched_files, key=lambda x: str(x).lower())\n",
    "\n",
    "    for f in matched_files:\n",
    "        print(\"   \", f)\n",
    "    print(\"-\" * 60)\n",
    "    return matched_files\n",
    "\n",
    "\n",
    "# === 第2步：并行生成 Base64 下载链接 ===\n",
    "import base64\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def generate_base64_link(file_path):\n",
    "    \"\"\"\n",
    "    读取单个文件并生成 Base64 下载链接\n",
    "    \"\"\"\n",
    "    file_name = os.path.basename(file_path)\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        encoded = base64.b64encode(f.read()).decode()\n",
    "    return f'<a download=\"{file_path}\" href=\"data:application/zip;base64,{encoded}\">下载 {file_name}</a>'\n",
    "\n",
    "def create_base64_links_parallel(files, max_workers=8):\n",
    "    \"\"\"\n",
    "    并行生成 Base64 下载链接并显示\n",
    "    \"\"\"\n",
    "    if not files:\n",
    "        return\n",
    "\n",
    "    print(f\" 开始并行生成 {len(files)} 个文件的 Base64 链接...\")\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(generate_base64_link, f): f for f in files}\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            file_path = futures[future]\n",
    "            try:\n",
    "                link_html = future.result()\n",
    "                display(HTML(link_html))\n",
    "            except Exception as e:\n",
    "                print(f\"  生成 {file_path} 链接失败: {e}\")\n",
    "\n",
    "    print(f\" 全部完成，共生成 {len(files)} 个链接。\")\n",
    "\n",
    "def split_list(lst, chunk_size):\n",
    "    \"\"\"将列表按指定大小分块\"\"\"\n",
    "    for i in range(0, len(lst), chunk_size):\n",
    "        yield lst[i:i + chunk_size]\n",
    "\n",
    "file_list=[]\n",
    "file_list += find_files(\"/QuantConnect/research-cloud/airlock/footprint_data\", pattern=rf\"{year}.*\", max_depth=2)\n",
    "\n",
    "batches = list(split_list(file_list, 20))\n",
    "\n",
    "print(f\"共计 {len(file_list)} 个匹配文件\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_base64_links_parallel(batches[0], max_workers=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Foundation-Py-Default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
